{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel \n",
    "import torch \n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-3B\",\n",
    "    load_in_4bit = True , # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PHOENIX-2014-T dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def create_phoenix14t_dataset(base_path):\n",
    "    \"\"\"\n",
    "    Create a dataset from Phoenix14T videos\n",
    "    \n",
    "    Args:\n",
    "        base_path: Path to the PHOENIX-2014-T directory containing train/dev/test splits\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Process each split (train, dev, test)\n",
    "    for split in [\"train\", \"dev\", \"test\"]:\n",
    "        split_path = os.path.join(base_path, \"PHOENIX-2014-T/features/fullFrame-210x260px\", split)\n",
    "        \n",
    "        # Get all MP4 files in the split directory\n",
    "        video_files = [f for f in os.listdir(split_path) if f.endswith('.mp4')]\n",
    "        \n",
    "        # Create a list of dictionaries containing video paths\n",
    "        data = {\n",
    "            'video_path': [os.path.join(split_path, vid) for vid in video_files],\n",
    "            'video_name': [os.path.splitext(vid)[0] for vid in video_files]\n",
    "        }\n",
    "        \n",
    "        # Convert to DataFrame first (easier to handle)\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Convert DataFrame to Dataset\n",
    "        datasets[split] = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Create a DatasetDict containing all splits\n",
    "    dataset_dict = DatasetDict(datasets)\n",
    "    return dataset_dict\n",
    "\n",
    "# Usage example:\n",
    "base_path = \"../PHOENIX-2014-T-release-v3/\"  # Adjust this path to your dataset location\n",
    "dataset = create_phoenix14t_dataset(base_path)\n",
    "\n",
    "# Now you can access different splits like:\n",
    "train_dataset = dataset['train']\n",
    "dev_dataset = dataset['dev']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Print some information about the dataset\n",
    "print(f\"Train set size: {len(dataset['train'])}\")\n",
    "print(f\"Dev set size: {len(dataset['dev'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")\n",
    "\n",
    "# Example of accessing a single item\n",
    "print(\"\\nExample item from training set:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_video\n",
    "\n",
    "def load_video(example):\n",
    "    \"\"\"Load video for a single example\"\"\"\n",
    "    video, audio, info = read_video(example['video_path'])\n",
    "    # Preprocess video if needed (resize, normalize, etc.)\n",
    "    example['video'] = video\n",
    "    return example\n",
    "\n",
    "# Apply the loading function to the dataset\n",
    "train_sign_dataset = train_dataset.map(load_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
    "    train_dataset = train_sign_dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",     # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 2048,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
