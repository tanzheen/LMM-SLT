#!/bin/bash
#PBS -N QwenSLT
#PBS -l select=1:ncpus=24:mpiprocs=1:ompthreads=24:mem=250gb:ngpus=1
#PBS -l walltime=02:00:00

cd $PBS_O_WORKDIR; 
cd LMM-SLT

image=/app1/common/singularity-img/hopper/pytorch/pytorch_2.4.0a0-cuda_12.5.0_ngc_24.06.sif

singularity exec $image bash << EOF > stdout.$PBS_JOBID 2> stderr.$PBS_JOBID
export PATH=$PATH:/home/svu/e0724993/.local/bin
export PYTHONPATH=$PYTHONPATH:/home/svu/e0724993/.local/lib/python3.10/site-packages
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# Clear GPU cache before starting
python -c "import torch; torch.cuda.empty_cache()"

# Verify the DeepSpeed config file
echo "Checking DeepSpeed config file:"
cat scripts/zero3_offload.json

deepspeed training/train.py \
    --deepspeed scripts/zero3_offload.json \
    --model_id Qwen2.5 \
    --train_data_path ./data/Phonexi-2014T/labels.train \
    --eval_data_path ./data/Phonexi-2014T/labels.dev \
    --image_folder /scratch/e0724993/PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/ \
    --remove_unused_columns False \
    --freeze_vision_tower False \
    --freeze_llm True \
    --tune_merger True \
    --bf16 False \
    --fp16 True  \
    --disable_flash_attn2 True \
    --output_dir output/testing_lora \
    --num_train_epochs 10 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 16 \
    --image_min_pixels $((256 * 28 * 28)) \
    --image_max_pixels $((512 * 28 * 28)) \
    --learning_rate 1e-4 \
    --merger_lr 1e-5 \
    --vision_lr 2e-6 \
    --weight_decay 0.1 \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps 10 \
    --tf32 True \
    --gradient_checkpointing True \
    --report_to tensorboard \
    --lazy_preprocess True \
    --save_strategy "steps" \
    --save_steps 200 \
    --save_total_limit 10 \
    --dataloader_num_workers 2 \
    --lora_enable True \
    --use_dora False \
    --lora_rank 32 \
    --lora_alpha 32 \
    --lora_dropout 0.05 \
    --num_lora_modules -1
EOF